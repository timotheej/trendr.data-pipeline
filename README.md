# ğŸš€ Trendr Data Pipeline - Production Ready

## Overview

SystÃ¨me autonome d'ingestion et classification POIs optimisÃ© pour **coÃ»t minimal** et **donnÃ©es fraÃ®ches**.

### ğŸ¯ FonctionnalitÃ©s ClÃ©s

- **Rotation par arrondissements** : DÃ©tection nouveaux POIs (1.89â‚¬/mois pour Paris)
- **Classification sociale intelligente** : Tags basÃ©s sur social proofs + Michelin, mÃ©dias
- **Collections dynamiques** : 7 types ("Romantic Date Spots", "Instagram-Worthy", etc.)
- **SystÃ¨me autonome** : 24/7 sans intervention
- **OptimisÃ© API coÃ»ts** : 92% rÃ©duction vs systÃ¨me prÃ©cÃ©dent

## ğŸƒâ€â™‚ï¸ Quick Start

### 1. Installation
```bash
pip install -r requirements.txt
```

### 2. Configuration
CrÃ©er `.env` avec :
```
SUPABASE_URL=your_supabase_url
SUPABASE_KEY=your_supabase_key
GOOGLE_PLACES_API_KEY=your_places_key
GOOGLE_CUSTOM_SEARCH_API_KEY=your_search_key
GOOGLE_CUSTOM_SEARCH_ENGINE_ID=your_engine_id
```

### 3. Lancement Autonome

#### SystÃ¨me AutomatisÃ© (RecommandÃ©)
```bash
# DÃ©marrer le systÃ¨me de monitoring 24/7
./start-monitoring.sh daemon

# ArrÃªter le systÃ¨me
./stop-monitoring.sh

# Voir la documentation complÃ¨te
cat AUTOMATION.md
```

#### ExÃ©cution Manuelle
```bash
# Pipeline complet 
python run_pipeline.py --city Paris --mode full

# Classification seulement
python run_pipeline.py --city Paris --mode classification

# Collections seulement  
python run_pipeline.py --city Paris --mode collections
```

## ğŸ“Š Architecture OptimisÃ©e

### Rotation Intelligente
- **Jour 1** : 1er arrondissement â†’ **Jour 20** : 20Ã¨me arrondissement
- **DÃ©tection nouveaux POIs** via comparaison place_ids
- **CoÃ»t quotidien** : ~0.06â‚¬ (4 catÃ©gories Ã— 3 POIs Ã— $0.017)

### Classification Social Proof
```python
# Tags dÃ©tectÃ©s automatiquement
trendy_tags = ["michelin_mentioned", "media_featured", "photo-worthy"]
hidden_gem_tags = ["local-favorite", "authentic", "wine_specialist"] 
```

### Collections BasÃ©es Tags
- **Romantic Date Spots** : `date-spot + intimate`
- **Instagram-Worthy** : `photo-worthy + trendy`  
- **Locals Only** : `local-favorite - tourist-friendly`

## ğŸ› ï¸ Scripts Utiles

### Ingestion CiblÃ©e
```bash
# Test rotation avec quartier spÃ©cifique
python scripts/google_places_ingester.py --test

# Quartier spÃ©cifique
python scripts/google_places_ingester.py --neighborhood "Marais"
```

### Classification
```bash
# Classification test (3 POIs)
python scripts/intelligent_classifier.py --test

# POI spÃ©cifique
python scripts/intelligent_classifier.py --poi-name "Breizh CafÃ©"
```

### Collections
```bash
# GÃ©nÃ©rer collections pour Paris
python ai/collection_generator.py --city Paris
```

### Maintenance
```bash
# Nettoyage base de donnÃ©es
python scripts/cleanup_database.py --analyze-only
python scripts/cleanup_database.py --execute
```

## ğŸ“ˆ Performance Production

### CoÃ»ts OptimisÃ©s
- **Ancien systÃ¨me** : ~540â‚¬/mois pour 216 POIs
- **Nouveau systÃ¨me** : ~45â‚¬/mois pour mÃªme coverage
- **RÃ©duction** : 92%

### ScalabilitÃ©
- **Paris** : 1.89â‚¬/mois (20 arrondissements)
- **+ Lyon** : +1.89â‚¬/mois (9 arrondissements) 
- **+ Marseille** : +1.89â‚¬/mois (16 arrondissements)

### RÃ©sultats Typiques (par jour)
- **11 nouveaux POIs** ingÃ©rÃ©s
- **Photos automatiques** pour chaque POI
- **Classification** avec confidence 70%+
- **Collections** mises Ã  jour automatiquement

## ğŸš€ DÃ©ploiement

### Docker
```bash
./docker-start.sh
```

### Cron Job (RecommandÃ©)
```bash
# Crontab : tous les jours Ã  2h du matin
0 2 * * * cd /path/to/trendr && python run_pipeline.py --city Paris --mode full >> daily_pipeline.log 2>&1
```

## ğŸ”§ Configuration AvancÃ©e

### `config.json` Production
```json
{
  "pipeline_config": {
    "daily_api_limit": 950,
    "batch_size": 25,
    "max_pois_per_category": 150
  },
  "api_cost_optimization": {
    "optimized_fields": "place_id,name,formatted_address,geometry,types,rating,user_ratings_total,website,price_level",
    "max_api_updates_per_day": 20
  }
}
```

## ğŸ“ Structure Projet

```
trendr-data-pipeline/
â”œâ”€â”€ ğŸš€ run_pipeline.py          # Point d'entrÃ©e principal
â”œâ”€â”€ ğŸ“Š config.json              # Configuration optimisÃ©e
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ google_places_ingester.py    # Ingestion rotation
â”‚   â”œâ”€â”€ intelligent_classifier.py    # Classification sociale
â”‚   â””â”€â”€ cleanup_database.py          # Maintenance
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ database.py              # Manager Supabase optimisÃ©
â”‚   â””â”€â”€ photo_manager.py         # Gestion photos automatique
â””â”€â”€ ai/
    â””â”€â”€ collection_generator.py  # Collections dynamiques
```

## âœ… Monitoring

### Logs Importants
```bash
tail -f daily_pipeline.log  # Pipeline principal
tail -f data_pipeline.log   # DÃ©tails techniques
```

### MÃ©triques Supabase
- **POIs table** : 239+ entrÃ©es
- **Collections table** : 7+ collections actives
- **Proof_sources table** : 758+ social proofs

## ğŸ¯ PrÃªt pour Production

Le systÃ¨me est **autonome**, **cost-efficient**, et **scalable**. 

**Lancement production** :
```bash
python run_pipeline.py --city Paris --mode full
```

---

*DÃ©veloppÃ© pour Trendr - Fresh data, social proof driven* ğŸš€